{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7335758-7d79-4642-8d5b-20b80889f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed4495-533e-49a0-9ea6-36c8ec20ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Installs if needed ===\n",
    "# pip install transformers accelerate bitsandbytes seaborn numpy torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01bf708-ede9-479f-97a6-9634dd099002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7d52e-172e-45f9-90d0-5064a46bbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Global Helper Functions ===\n",
    "def normalized_entropy(ps, axis=1, epsilon=1e-16):\n",
    "    \"\"\"Numerically stable normalized entropy\"\"\"\n",
    "    # Add epsilon to prevent zeros\n",
    "    ps = np.array(ps) + epsilon\n",
    "    ps = ps / np.expand_dims(np.sum(ps, axis=axis), 1)\n",
    "    \n",
    "    # Compute log safely\n",
    "    log_ps = np.where(ps > 0, np.log(ps), 0)\n",
    "    \n",
    "    # Calculate normalized entropy\n",
    "    seq_len = ps.shape[1]\n",
    "    max_entropy = np.log(seq_len)\n",
    "    return -np.sum(ps * log_ps, axis=1) / max_entropy\n",
    "\n",
    "def normalized_entropy_no_start(ps, axis=1, epsilon=1e-16):\n",
    "    \"\"\"\n",
    "    Calculates normalized entropy while excluding the start token's contribution.\n",
    "    \n",
    "    Args:\n",
    "        ps: Attention weights matrix of shape (n_heads, seq_len)\n",
    "        axis: Axis along which to normalize (default=1 for sequence dimension)\n",
    "        epsilon: Small value to avoid log(0)\n",
    "        \n",
    "    Returns:\n",
    "        Normalized entropy values with start token excluded\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    ps_no_start = np.array(ps.copy())\n",
    "    \n",
    "    # Zero out attention to the start token (index 0)\n",
    "    ps_no_start[:, 0] = 0\n",
    "    \n",
    "    # Renormalize the remaining attention weights to sum to 1\n",
    "    ps_no_start = ps_no_start + epsilon\n",
    "    ps_no_start = ps_no_start / np.expand_dims(np.sum(ps_no_start, axis=axis), 1)\n",
    "    \n",
    "    # Compute log safely\n",
    "    log_ps = np.where(ps_no_start > 0, np.log(ps_no_start), 0)\n",
    "    \n",
    "    # Calculate normalized entropy (excluding start token from sequence length)\n",
    "    seq_len = ps_no_start.shape[1] - 1  # Subtract 1 to exclude start token\n",
    "    max_entropy = np.log(seq_len)\n",
    "    \n",
    "    return -np.sum(ps_no_start * log_ps, axis=1) / max_entropy\n",
    "\n",
    "\n",
    "def process_attn_entropies(attentions, output_token=0):\n",
    "    '''\n",
    "    Simplified version for fixed-length truncated sequences\n",
    "    '''\n",
    "    entropies = []\n",
    "    for i, attn in enumerate(attentions[output_token]):\n",
    "        attn = attn.to(torch.float32).squeeze().cpu().numpy()\n",
    "        # Focus on last token (same for all sequences)\n",
    "        entropies.append(normalized_entropy(attn[:, -1, :], axis=1)) \n",
    "    return entropies\n",
    "\n",
    "def calculate_entropies(sentences):\n",
    "    entropies = []\n",
    "    for sentence in sentences:\n",
    "        model_inputs = tokenizer.encode(sentence, return_tensors=\"pt\").to(model.device)\n",
    "        output = model.generate(\n",
    "            model_inputs,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "        entropies.append(process_attn_entropies(output.attentions))\n",
    "    return np.array(entropies)\n",
    "\n",
    "def process_attn_entropies_no_start(attentions, output_token=0):\n",
    "    '''\n",
    "    Calculates attention entropy while excluding the start token.\n",
    "    '''\n",
    "    entropies = []\n",
    "    for i, attn in enumerate(attentions[output_token]):\n",
    "        attn = attn.to(torch.float32).squeeze().cpu().numpy()\n",
    "        # Focus on last token but exclude start token from entropy calculation\n",
    "        entropies.append(normalized_entropy_no_start(attn[:, -1, :], axis=1)) \n",
    "    return entropies\n",
    "\n",
    "def calculate_entropies_no_start(sentences):\n",
    "    entropies = []\n",
    "    for sentence in sentences:\n",
    "        model_inputs = tokenizer.encode(sentence, return_tensors=\"pt\").to(model.device)\n",
    "        output = model.generate(\n",
    "            model_inputs,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "        entropies.append(process_attn_entropies_no_start(output.attentions))\n",
    "    return np.array(entropies)\n",
    "\n",
    "\n",
    "def save_entropy_arrays(model_tag, dataset_tag, **arrays):\n",
    "    \"\"\"\n",
    "    Saves each array in `arrays` as a .npy file with a standardized filename.\n",
    "    \n",
    "    Args:\n",
    "        model_tag (str): Short identifier for the model, e.g., 'llama2_70b'\n",
    "        dataset_tag (str): Short identifier for the dataset, e.g., 'ling' or 'md'\n",
    "        arrays (dict): Named arrays to save, e.g., sensible=arr1, nonsensical=arr2\n",
    "    \"\"\"\n",
    "    save_dir = f\"entropy_results_{model_tag}_{dataset_tag}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for name, arr in arrays.items():\n",
    "        fname = f\"{save_dir}/{name}_entropies.npy\"\n",
    "        np.save(fname, arr)\n",
    "        print(f\"Saved: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e28804-d05c-4a08-b2fd-89b083344bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "model_name = \"meta-llama/CodeLlama-70b-Instruct-hf\" # Edit for model you want to initialize: meta-llama/Llama-2-70b-hf, meta-llama/Llama-2-70b-chat-hf, meta-llama/CodeLlama-70b-Instruct-hf, meta-llama/CodeLlama-70b-hf\n",
    "access_token = \"hf_acces_token\"\n",
    "\n",
    "\n",
    "# === Main Pipeline ===\n",
    "# Initialize model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e21358-cb72-4d7f-97a8-f2ea5627386f",
   "metadata": {},
   "source": [
    "## Linguistic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc9ca6-5d7d-40dc-8a77-cfc71cb31e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper Functions - Linguistic Dataset ===\n",
    "\n",
    "def _find_best_truncation_point(sentence, max_len, tokenizer):\n",
    "    \"\"\"\n",
    "    Helper function: Finds the latest possible token position (<= max_len)\n",
    "    that ends on a word boundary (space or punctuation).\n",
    "    Returns the number of tokens to keep.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer.encode_plus(sentence, add_special_tokens=False, return_offsets_mapping=True)\n",
    "    tokens = encoded.input_ids\n",
    "    offsets = encoded.offset_mapping\n",
    "\n",
    "    # Determine the upper limit for searching for a boundary\n",
    "    search_limit = min(len(tokens), max_len)\n",
    "\n",
    "    # Iterate backwards from the search limit to find the last valid boundary\n",
    "    for i in range(search_limit, 0, -1):\n",
    "        # Offset corresponds to the end character position of the (i-1)th token\n",
    "        end_char_pos = offsets[i-1][1]\n",
    "        # Check if the character after the token end is a boundary character\n",
    "        # Make sure we don't index out of bounds for the original sentence string\n",
    "        if end_char_pos < len(sentence) and sentence[end_char_pos] in [' ', '.', '!', '?', ',', ';', '\\n', '\\t']:\n",
    "            return i # Return the number of tokens to keep (ending at this boundary)\n",
    "        # Also consider the case where the token itself is the very end of the string\n",
    "        if end_char_pos == len(sentence):\n",
    "             return i # Keep tokens up to the end\n",
    "\n",
    "    # Let's restart the logic slightly for clarity:\n",
    "    # Find the last boundary <= search_limit\n",
    "    best_trunc_pos = 0 # Default to keeping nothing if no boundary found\n",
    "    for i in range(1, search_limit + 1): # Iterate up to the limit\n",
    "        end_char_pos = offsets[i-1][1]\n",
    "        is_boundary = False\n",
    "        if end_char_pos == len(sentence): # End of string is a boundary\n",
    "            is_boundary = True\n",
    "        elif end_char_pos < len(sentence) and sentence[end_char_pos] in [' ', '.', '!', '?', ',', ';', '\\n', '\\t']:\n",
    "            is_boundary = True\n",
    "\n",
    "        if is_boundary:\n",
    "            best_trunc_pos = i # Update the best position found so far\n",
    "\n",
    "    return best_trunc_pos\n",
    "\n",
    "\n",
    "def _truncate_sentence_to_tokens(sentence, num_tokens, tokenizer):\n",
    "    \"\"\"\n",
    "    Helper function: Truncates a sentence to exactly num_tokens\n",
    "    and adds ellipsis if needed.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer.encode_plus(sentence, add_special_tokens=False)\n",
    "    tokens = encoded.input_ids\n",
    "\n",
    "    truncated_tokens = tokens[:num_tokens]\n",
    "    adjusted = tokenizer.decode(truncated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Add ellipsis if actual truncation occurred and the result is not empty\n",
    "    if num_tokens < len(tokens) and adjusted and adjusted[-1] not in ['.', '!', '?']:\n",
    "        adjusted += '...'\n",
    "\n",
    "    return adjusted\n",
    "\n",
    "\n",
    "def smart_truncate(sensible_sentences, nonsensical_sentences, tokenizer):\n",
    "    \"\"\"\n",
    "    Truncates pairs of sentences (sensible, nonsensical) to a common token length\n",
    "    determined by the shorter sentence within the pair, while preserving complete words.\n",
    "\n",
    "    Args:\n",
    "        sensible_sentences (list): List of sensible sentences.\n",
    "        nonsensical_sentences (list): List of nonsensical sentences (must be same length).\n",
    "        tokenizer: The tokenizer instance.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "               (truncated_sensible_sentences, truncated_nonsensical_sentences)\n",
    "    \"\"\"\n",
    "    if len(sensible_sentences) != len(nonsensical_sentences):\n",
    "        raise ValueError(\"Input lists must have the same length for pairwise truncation.\")\n",
    "\n",
    "    truncated_sensible = []\n",
    "    truncated_nonsensical = []\n",
    "\n",
    "    print(f\"Processing {len(sensible_sentences)} sentence pairs for pairwise truncation...\")\n",
    "\n",
    "    for i, (sensible_s, nonsensical_s) in enumerate(zip(sensible_sentences, nonsensical_sentences)):\n",
    "        # 1. Get original token lengths for the pair\n",
    "        len_s = len(tokenizer.encode(sensible_s, add_special_tokens=False))\n",
    "        len_n = len(tokenizer.encode(nonsensical_s, add_special_tokens=False))\n",
    "\n",
    "        # 2. Determine the target maximum length for this pair\n",
    "        target_max_len = min(len_s, len_n)\n",
    "\n",
    "        # 3. Find the best actual truncation point (<= target_max_len) for BOTH sentences\n",
    "        trunc_pos_s = _find_best_truncation_point(sensible_s, target_max_len, tokenizer)\n",
    "        trunc_pos_n = _find_best_truncation_point(nonsensical_s, target_max_len, tokenizer)\n",
    "\n",
    "        # 4. The final length for the pair is the minimum of the two valid points\n",
    "        #    This ensures *both* sentences can be truncated cleanly to the *same* length.\n",
    "        final_len_pair = min(trunc_pos_s, trunc_pos_n)\n",
    "\n",
    "        # 5. Truncate both sentences to this final agreed-upon length\n",
    "        final_sensible = _truncate_sentence_to_tokens(sensible_s, final_len_pair, tokenizer)\n",
    "        final_nonsensical = _truncate_sentence_to_tokens(nonsensical_s, final_len_pair, tokenizer)\n",
    "\n",
    "        truncated_sensible.append(final_sensible)\n",
    "        truncated_nonsensical.append(final_nonsensical)\n",
    "\n",
    "        # Optional: Progress indicator\n",
    "        if (i + 1) % 50 == 0:\n",
    "             print(f\"  Processed {i+1}/{len(sensible_sentences)} pairs...\")\n",
    "\n",
    "    print(\"Pairwise truncation complete.\")\n",
    "    return truncated_sensible, truncated_nonsensical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b7355-265d-49b8-8ab2-3dae091fcc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data (all sentences)\n",
    "try:\n",
    "    df = pd.read_csv('linguistic_dataset_full.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'linguistic_dataset_full.csv' not found. Please ensure the file is in the correct directory.\")\n",
    "    raise\n",
    "\n",
    "# Filter sensible and nonsensical sentences\n",
    "all_sensible_df = df[df['type'] == 'S']  # DataFrame of all sensible sentences\n",
    "all_nonsensical_df = df[df['type'] == 'N']  # DataFrame of all nonsensical sentences\n",
    "\n",
    "# --- Match pairs based on sentence_id ---\n",
    "# Merge the dataframes to easily align sensible and nonsensical sentences by their ID\n",
    "# This ensures we are comparing the correct pairs\n",
    "merged_df = pd.merge(all_sensible_df, all_nonsensical_df, on='sentence_id', suffixes=('_sensible', '_nonsensical'))\n",
    "\n",
    "# Extract the paired sentences and their IDs into aligned lists\n",
    "sensible_sentences_original = merged_df['sentence_sensible'].tolist()\n",
    "nonsensical_sentences_original = merged_df['sentence_nonsensical'].tolist()\n",
    "pair_ids = merged_df['sentence_id'].tolist() # Keep track of IDs if needed later\n",
    "num_pairs = len(pair_ids) # Total number of complete pairs found\n",
    "\n",
    "# --- Apply PAIRWISE smart_truncate ---\n",
    "# This function takes the two aligned lists and returns two truncated aligned lists\n",
    "sensible_sentences_truncated, nonsensical_sentences_truncated = smart_truncate(\n",
    "    sensible_sentences_original,\n",
    "    nonsensical_sentences_original,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# --- Assign truncated sentences for downstream use ---\n",
    "# These are the variables subsequent cells will use\n",
    "sensible_sentences = sensible_sentences_truncated\n",
    "nonsensical_sentences = nonsensical_sentences_truncated\n",
    "\n",
    "# --- Calculate and report token lengths AFTER pairwise truncation ---\n",
    "# Note: Lengths within a pair will be identical, but lengths might vary across pairs.\n",
    "sensible_token_lengths_after = [len(tokenizer.encode(s, add_special_tokens=False)) for s in sensible_sentences]\n",
    "nonsensical_token_lengths_after = [len(tokenizer.encode(s, add_special_tokens=False)) for s in nonsensical_sentences]\n",
    "\n",
    "# Calculate min and max lengths ACROSS ALL PAIRS after truncation\n",
    "sensible_min_len_after = min(sensible_token_lengths_after) if sensible_token_lengths_after else None\n",
    "sensible_max_len_after = max(sensible_token_lengths_after) if sensible_token_lengths_after else None\n",
    "# nonsensical min/max will be the same because lengths match within pairs\n",
    "nonsensical_min_len_after = min(nonsensical_token_lengths_after) if nonsensical_token_lengths_after else None\n",
    "nonsensical_max_len_after = max(nonsensical_token_lengths_after) if nonsensical_token_lengths_after else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15b847-c79f-4ddf-9def-87b478802b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Attention Sink Detection Functions ===\n",
    "def normalized_entropy_method1(ps, epsilon=1e-16):\n",
    "    \"\"\"\n",
    "    Numerically stable normalized entropy calculation\n",
    "    \n",
    "    \"\"\"\n",
    "    # Add epsilon to prevent zeros\n",
    "    ps = np.array(ps) + epsilon\n",
    "    ps = ps / np.sum(ps)  # Renormalize to ensure sum = 1\n",
    "    \n",
    "    # Compute log safely\n",
    "    log_ps = np.where(ps > 0, np.log(ps), 0)\n",
    "    \n",
    "    # Calculate normalized entropy\n",
    "    seq_len = len(ps)\n",
    "    max_entropy = np.log(seq_len)\n",
    "    return -np.sum(ps * log_ps) / max_entropy\n",
    "\n",
    "def detect_attention_sinks_weighted(attention_weights, normalize_by_length=True):\n",
    "    \"\"\"\n",
    "    Detect attention sinks and calculate weighted entropy.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Focus on the last query token (the one we're generating)\n",
    "    last_token_attention = attention_weights[:, -1, :]  # Shape: (n_heads, seq_len_key)\n",
    "    \n",
    "    n_heads, seq_len = last_token_attention.shape\n",
    "    \n",
    "    sink_strengths = []\n",
    "    weighted_entropies = []\n",
    "    standard_entropies = []\n",
    "    \n",
    "    for head_idx in range(n_heads):\n",
    "        probs = last_token_attention[head_idx]  # P1, P2, ..., Pn\n",
    "        \n",
    "        # P1 is attention to start-of-sequence token (first token)\n",
    "        p1 = probs[0]\n",
    "        \n",
    "        # P2, P3, ..., Pn are attention to content tokens\n",
    "        content_probs = probs[1:]\n",
    "        \n",
    "        # Calculate standard entropy using Method 1's approach\n",
    "        standard_entropy = normalized_entropy_method1(probs)\n",
    "        \n",
    "        # Calculate weighted entropy\n",
    "        # weighted entropy = (1-P1) * Σ(i=2 to n) Pi * log(Pi)\n",
    "        if len(content_probs) > 0:\n",
    "            content_entropy = normalized_entropy_method1(content_probs)\n",
    "            # Convert back to unnormalized\n",
    "            content_entropy_unnormalized = content_entropy * np.log(len(content_probs))\n",
    "            weighted_entropy = (1 - p1) * content_entropy_unnormalized\n",
    "            \n",
    "            # Normalize weighted entropy by content length\n",
    "            if normalize_by_length and len(content_probs) > 1:\n",
    "                weighted_entropy = weighted_entropy / np.log(len(content_probs))\n",
    "        else:\n",
    "            weighted_entropy = 0.0\n",
    "        \n",
    "        sink_strengths.append(p1)\n",
    "        weighted_entropies.append(weighted_entropy)\n",
    "        standard_entropies.append(standard_entropy)\n",
    "    \n",
    "    return np.array(sink_strengths), np.array(weighted_entropies), np.array(standard_entropies)\n",
    "\n",
    "def analyze_attention_patterns_weighted(attention_weights):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis using weighted entropy method.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Calculate weighted entropies using supervisor's method\n",
    "    sink_strengths, weighted_entropies, standard_entropies = detect_attention_sinks_weighted(attention_weights)\n",
    "    \n",
    "    # Additional statistics\n",
    "    last_token_attention = attention_weights[:, -1, :]\n",
    "    max_attention_per_head = np.max(last_token_attention, axis=1)\n",
    "    attention_concentration = np.sum(last_token_attention**2, axis=1)\n",
    "    \n",
    "    # Binary sink classification for comparison (using 0.9 threshold)\n",
    "    binary_sink_heads = sink_strengths > 0.9\n",
    "    \n",
    "    return {\n",
    "        'sink_strengths': sink_strengths,  # P1 values\n",
    "        'weighted_entropies': weighted_entropies,  \n",
    "        'standard_entropies': standard_entropies,  # For comparison\n",
    "        'binary_sink_heads': binary_sink_heads,  # Binary classification\n",
    "        'max_attention_per_head': max_attention_per_head,\n",
    "        'attention_concentration': attention_concentration,\n",
    "        'raw_attention': last_token_attention\n",
    "    }\n",
    "\n",
    "# --- Modified Processing Functions ---\n",
    "def process_attn_entropies_with_weighted_analysis(attentions, output_token=0):\n",
    "    \"\"\"\n",
    "    Process attention weights weighted entropy method.\n",
    "    \n",
    "    \"\"\"\n",
    "    weighted_entropies = []\n",
    "    standard_entropies = []\n",
    "    sink_analyses = []\n",
    "    \n",
    "    for i, attn in enumerate(attentions[output_token]):\n",
    "        attn_numpy = attn.to(torch.float32).squeeze().cpu().numpy()\n",
    "        # Shape: (n_heads, seq_len_query, seq_len_key)\n",
    "        \n",
    "        # Analyze using weighted method\n",
    "        sink_analysis = analyze_attention_patterns_weighted(attn_numpy)\n",
    "        sink_analyses.append(sink_analysis)\n",
    "        \n",
    "        # Extract entropies\n",
    "        weighted_entropies.append(sink_analysis['weighted_entropies'])\n",
    "        standard_entropies.append(sink_analysis['standard_entropies'])\n",
    "    \n",
    "    return np.array(weighted_entropies), np.array(standard_entropies), sink_analyses\n",
    "\n",
    "def calculate_weighted_entropies_with_sink_analysis(input_texts):\n",
    "    \"\"\"\n",
    "    Calculate weighted entropies for a list of input texts.\n",
    "    Works for both sentences and mathematical problems.\n",
    "    \n",
    "    \"\"\"\n",
    "    weighted_entropies = []\n",
    "    standard_entropies = []\n",
    "    all_sink_analyses = []\n",
    "    \n",
    "    for text in input_texts:\n",
    "        model_inputs = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "        output = model.generate(\n",
    "            model_inputs,\n",
    "            output_attentions=True,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "        \n",
    "        # Process with weighted analysis\n",
    "        text_weighted, text_standard, text_sink_analyses = process_attn_entropies_with_weighted_analysis(output.attentions)\n",
    "        \n",
    "        weighted_entropies.append(text_weighted)\n",
    "        standard_entropies.append(text_standard)\n",
    "        all_sink_analyses.append(text_sink_analyses)\n",
    "    \n",
    "    return np.array(weighted_entropies), np.array(standard_entropies), all_sink_analyses\n",
    "\n",
    "# === Analysis Functions ===\n",
    "def summarize_weighted_sink_behavior(sink_analyses, condition_name):\n",
    "    \"\"\"\n",
    "    Summarize attention sink behavior using weighted entropy method.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"\\n=== WEIGHTED ENTROPY ANALYSIS: {condition_name} ===\")\n",
    "    \n",
    "    # Aggregate across all inputs and layers\n",
    "    all_sink_strengths = []\n",
    "    all_weighted_entropies = []\n",
    "    all_standard_entropies = []\n",
    "    \n",
    "    for input_analysis in sink_analyses:\n",
    "        for layer_analysis in input_analysis:\n",
    "            all_sink_strengths.extend(layer_analysis['sink_strengths'])\n",
    "            all_weighted_entropies.extend(layer_analysis['weighted_entropies'])\n",
    "            all_standard_entropies.extend(layer_analysis['standard_entropies'])\n",
    "    \n",
    "    all_sink_strengths = np.array(all_sink_strengths)\n",
    "    all_weighted_entropies = np.array(all_weighted_entropies)\n",
    "    all_standard_entropies = np.array(all_standard_entropies)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_sink_strength = np.mean(all_sink_strengths)\n",
    "    mean_weighted_entropy = np.mean(all_weighted_entropies)\n",
    "    mean_standard_entropy = np.mean(all_standard_entropies)\n",
    "    \n",
    "    # Binary classification for comparison\n",
    "    strong_sinks = all_sink_strengths > 0.9\n",
    "    sink_percentage = np.mean(strong_sinks) * 100\n",
    "    \n",
    "    print(f\"Mean sink strength (P1): {mean_sink_strength:.3f}\")\n",
    "    print(f\"Mean weighted entropy: {mean_weighted_entropy:.3f}\")\n",
    "    print(f\"Mean standard entropy: {mean_standard_entropy:.3f}\")\n",
    "    print(f\"Strong sink heads (P1 > 0.9): {sink_percentage:.1f}%\")\n",
    "    print(f\"Entropy reduction from weighting: {mean_standard_entropy - mean_weighted_entropy:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'mean_sink_strength': mean_sink_strength,\n",
    "        'mean_weighted_entropy': mean_weighted_entropy,\n",
    "        'mean_standard_entropy': mean_standard_entropy,\n",
    "        'sink_percentage': sink_percentage,\n",
    "        'entropy_reduction': mean_standard_entropy - mean_weighted_entropy\n",
    "    }\n",
    "\n",
    "def compare_weighted_sink_behavior(sink_analyses_1, sink_analyses_2, condition1_name, condition2_name):\n",
    "    \"\"\"\n",
    "    Compare attention sink behavior between two conditions using weighted entropy.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"\\n=== COMPARING WEIGHTED SINK BEHAVIOR: {condition1_name} vs {condition2_name} ===\")\n",
    "    \n",
    "    summary1 = summarize_weighted_sink_behavior(sink_analyses_1, condition1_name)\n",
    "    summary2 = summarize_weighted_sink_behavior(sink_analyses_2, condition2_name)\n",
    "    \n",
    "    # Calculate differences\n",
    "    sink_diff = summary2['mean_sink_strength'] - summary1['mean_sink_strength']\n",
    "    weighted_entropy_diff = summary2['mean_weighted_entropy'] - summary1['mean_weighted_entropy']\n",
    "    standard_entropy_diff = summary2['mean_standard_entropy'] - summary1['mean_standard_entropy']\n",
    "    \n",
    "    print(f\"\\nDifferences ({condition2_name} - {condition1_name}):\")\n",
    "    print(f\"Sink strength difference: {sink_diff:+.3f}\")\n",
    "    print(f\"Weighted entropy difference: {weighted_entropy_diff:+.3f}\")\n",
    "    print(f\"Standard entropy difference: {standard_entropy_diff:+.3f}\")\n",
    "    \n",
    "    return summary1, summary2\n",
    "\n",
    "# --- Data Extraction Functions ---\n",
    "def extract_sink_strengths(sink_analyses):\n",
    "    \"\"\"\n",
    "    Extract sink strength values (P1).\n",
    "    Returns shape: (n_samples, n_layers, n_heads)\n",
    "    \n",
    "    \"\"\"\n",
    "    sink_strengths = []\n",
    "    for input_analysis in sink_analyses:\n",
    "        input_strengths = []\n",
    "        for layer_analysis in input_analysis:\n",
    "            input_strengths.append(layer_analysis['sink_strengths'])\n",
    "        sink_strengths.append(np.array(input_strengths))\n",
    "    return np.array(sink_strengths)\n",
    "\n",
    "def extract_weighted_entropies(sink_analyses):\n",
    "    \"\"\"\n",
    "    Extract weighted entropy values.\n",
    "    Returns shape: (n_samples, n_layers, n_heads)\n",
    "    \n",
    "    \"\"\"\n",
    "    weighted_entropies = []\n",
    "    for input_analysis in sink_analyses:\n",
    "        input_entropies = []\n",
    "        for layer_analysis in input_analysis:\n",
    "            input_entropies.append(layer_analysis['weighted_entropies'])\n",
    "        weighted_entropies.append(np.array(input_entropies))\n",
    "    return np.array(weighted_entropies)\n",
    "\n",
    "def extract_standard_entropies(sink_analyses):\n",
    "    \"\"\"\n",
    "    Extract standard entropy values for comparison.\n",
    "    Returns shape: (n_samples, n_layers, n_heads)\n",
    "    \n",
    "    \"\"\"\n",
    "    standard_entropies = []\n",
    "    for input_analysis in sink_analyses:\n",
    "        input_entropies = []\n",
    "        for layer_analysis in input_analysis:\n",
    "            input_entropies.append(layer_analysis['standard_entropies'])\n",
    "        standard_entropies.append(np.array(input_entropies))\n",
    "    return np.array(standard_entropies)\n",
    "\n",
    "def extract_binary_sink_heads(sink_analyses):\n",
    "    \"\"\"\n",
    "    Extract binary sink head classifications.\n",
    "    Returns shape: (n_samples, n_layers, n_heads)\n",
    "    \n",
    "    \"\"\"\n",
    "    binary_sinks = []\n",
    "    for input_analysis in sink_analyses:\n",
    "        input_binary = []\n",
    "        for layer_analysis in input_analysis:\n",
    "            input_binary.append(layer_analysis['binary_sink_heads'])\n",
    "        binary_sinks.append(np.array(input_binary))\n",
    "    return np.array(binary_sinks)\n",
    "\n",
    "# --- Save numpy files ---\n",
    "def save_weighted_entropy_arrays(model_tag, dataset_tag, **arrays):\n",
    "    \"\"\"\n",
    "    Save entropy arrays with flexible naming for different datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create directory\n",
    "    dir_name = f\"entropy_results_{model_tag}_{dataset_tag}\"\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    # Save each array\n",
    "    for array_name, array_data in arrays.items():\n",
    "        if array_data is not None:\n",
    "            file_path = os.path.join(dir_name, f\"{array_name}.npy\")\n",
    "            np.save(file_path, array_data)\n",
    "            print(f\"Saved {array_name}.npy with shape {array_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (MODIFY THIS FOR EACH MODEL) ---\n",
    "MODEL_TAG = \"codellama_instruct_70b\"  # Change this for each model: \"llama_2_70b_base\", \"codellama_70b\", \"codellama_instruct_70b\", \"llama_chat_70b\"\n",
    "DATASET_TAG = \"ling\"\n",
    "\n",
    "print(f\"Processing {MODEL_TAG} on linguistic dataset (sensible vs nonsensical)\")\n",
    "\n",
    "# --- Modified Main Analysis ---\n",
    "print(\"--- CALCULATING WEIGHTED ENTROPIES WITH ATTENTION SINK ANALYSIS ---\")\n",
    "\n",
    "# Calculate weighted entropies using supervisor's method\n",
    "print(\"Processing sensible sentences...\")\n",
    "sensible_weighted_entropies, sensible_standard_entropies, sensible_sink_analyses = calculate_weighted_entropies_with_sink_analysis(sensible_sentences)\n",
    "\n",
    "print(\"Processing nonsensical sentences...\")\n",
    "nonsensical_weighted_entropies, nonsensical_standard_entropies, nonsensical_sink_analyses = calculate_weighted_entropies_with_sink_analysis(nonsensical_sentences)\n",
    "\n",
    "# Extract data arrays\n",
    "print(\"Extracting sink strengths...\")\n",
    "sensible_sink_strengths = extract_sink_strengths(sensible_sink_analyses)\n",
    "nonsensical_sink_strengths = extract_sink_strengths(nonsensical_sink_analyses)\n",
    "\n",
    "print(\"Extracting weighted entropies...\")\n",
    "sensible_weighted_extracted = extract_weighted_entropies(sensible_sink_analyses)\n",
    "nonsensical_weighted_extracted = extract_weighted_entropies(nonsensical_sink_analyses)\n",
    "\n",
    "print(\"Extracting standard entropies...\")\n",
    "sensible_standard_extracted = extract_standard_entropies(sensible_sink_analyses)\n",
    "nonsensical_standard_extracted = extract_standard_entropies(nonsensical_sink_analyses)\n",
    "\n",
    "print(\"Extracting binary sink classifications...\")\n",
    "sensible_binary_sinks = extract_binary_sink_heads(sensible_sink_analyses)\n",
    "nonsensical_binary_sinks = extract_binary_sink_heads(nonsensical_sink_analyses)\n",
    "\n",
    "# Perform weighted entropy analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WEIGHTED ENTROPY BEHAVIOR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze each condition using supervisor's method\n",
    "sensible_summary = summarize_weighted_sink_behavior(sensible_sink_analyses, \"SENSIBLE SENTENCES\")\n",
    "nonsensical_summary = summarize_weighted_sink_behavior(nonsensical_sink_analyses, \"NONSENSICAL SENTENCES\")\n",
    "\n",
    "# Compare conditions\n",
    "compare_weighted_sink_behavior(sensible_sink_analyses, nonsensical_sink_analyses, \"Sensible\", \"Nonsensical\")\n",
    "\n",
    "# Save updated arrays\n",
    "save_weighted_entropy_arrays(\n",
    "    model_tag=MODEL_TAG,\n",
    "    dataset_tag=DATASET_TAG,\n",
    "    \n",
    "    # Weighted entropy data (supervisor's method)\n",
    "    sensible_weighted_entropy=sensible_weighted_extracted,\n",
    "    nonsensical_weighted_entropy=nonsensical_weighted_extracted,\n",
    "    \n",
    "    # Standard entropy for comparison (Method 1)\n",
    "    sensible_standard_entropy=sensible_standard_extracted,\n",
    "    nonsensical_standard_entropy=nonsensical_standard_extracted,\n",
    "    \n",
    "    # Sink strength values (P1)\n",
    "    sensible_sink_strength=sensible_sink_strengths,\n",
    "    nonsensical_sink_strength=nonsensical_sink_strengths,\n",
    "    \n",
    "    # Binary sink classifications\n",
    "    sensible_binary_sinks=sensible_binary_sinks,\n",
    "    nonsensical_binary_sinks=nonsensical_binary_sinks,\n",
    ")\n",
    "\n",
    "print(f\"\\nLinguistic weighted entropy analysis complete for {MODEL_TAG}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb0eda-ac34-4be9-99aa-7395e5c36204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_start_token(sentence):\n",
    "    tokens = tokenizer.encode(sentence, return_tensors=\"pt\")[0]\n",
    "    decoded_tokens = [tokenizer.decode([t]) for t in tokens]\n",
    "    print(f\"First token: '{decoded_tokens[0]}'\")\n",
    "    print(f\"Full tokenization: {decoded_tokens}\")\n",
    "    \n",
    "    # Check if first token is a special token\n",
    "    special_tokens = tokenizer.all_special_tokens\n",
    "    is_special = any(decoded_tokens[0] == st for st in special_tokens)\n",
    "    print(f\"Is first token a special token? {is_special}\")\n",
    "    return is_special\n",
    "\n",
    "# Test with a sample sentence\n",
    "verify_start_token(\"This is a test sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79ae53-36d1-4698-8220-0d130d4d0826",
   "metadata": {},
   "source": [
    "## Multimodal Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc2436-ac3f-40a4-8212-3aa58fd9deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (MODIFY THIS FOR EACH MODEL) ---\n",
    "MODEL_TAG = \"codellama_instruct_70b\"  # Change this for each model: \"llama_2_70b_base\", \"codellama_70b\", \"codellama_instruct_70b\"\n",
    "DATASET_TAG = \"md\"\n",
    "\n",
    "print(f\"Processing {MODEL_TAG} on MD dataset (easy vs hard)\")\n",
    "\n",
    "# --- MD Dataset Loading and Preprocessing ---\n",
    "md_df = pd.read_csv(\"MD_Dataset_4.csv\")\n",
    "\n",
    "# Filter the DataFrame into easy and hard problems\n",
    "md_easy_df = md_df[md_df['type'] == 'easy']\n",
    "md_hard_df = md_df[md_df['type'] == 'hard']\n",
    "\n",
    "print(f\"Total original easy problems found: {len(md_easy_df)}\")\n",
    "print(f\"Total original hard problems found: {len(md_hard_df)}\")\n",
    "\n",
    "# --- Match pairs based on pair_id ---\n",
    "merged_md_df = pd.merge(md_easy_df, md_hard_df, on='pair_id', suffixes=('_easy', '_hard'))\n",
    "\n",
    "# Extract the aligned problems and their IDs into lists\n",
    "problems_easy_original = merged_md_df['problem_easy'].tolist()\n",
    "problems_hard_original = merged_md_df['problem_hard'].tolist()\n",
    "pair_ids_md = merged_md_df['pair_id'].tolist()\n",
    "num_pairs_md = len(pair_ids_md)\n",
    "\n",
    "print(f\"Found and aligned {num_pairs_md} complete easy/hard problem pairs.\")\n",
    "\n",
    "# --- Modified Main Analysis ---\n",
    "print(\"--- CALCULATING WEIGHTED ENTROPIES WITH ATTENTION SINK ANALYSIS FOR MD DATASET ---\")\n",
    "\n",
    "# Calculate weighted entropies\n",
    "print(\"Processing easy problems...\")\n",
    "easy_weighted_entropies, easy_standard_entropies, easy_sink_analyses = calculate_weighted_entropies_with_sink_analysis(problems_easy_original)\n",
    "\n",
    "print(\"Processing hard problems...\")\n",
    "hard_weighted_entropies, hard_standard_entropies, hard_sink_analyses = calculate_weighted_entropies_with_sink_analysis(problems_hard_original)\n",
    "\n",
    "# Extract all data arrays\n",
    "print(\"Extracting sink strengths...\")\n",
    "easy_sink_strengths = extract_sink_strengths(easy_sink_analyses)\n",
    "hard_sink_strengths = extract_sink_strengths(hard_sink_analyses)\n",
    "\n",
    "print(\"Extracting weighted entropies...\")\n",
    "easy_weighted_extracted = extract_weighted_entropies(easy_sink_analyses)\n",
    "hard_weighted_extracted = extract_weighted_entropies(hard_sink_analyses)\n",
    "\n",
    "print(\"Extracting standard entropies...\")\n",
    "easy_standard_extracted = extract_standard_entropies(easy_sink_analyses)\n",
    "hard_standard_extracted = extract_standard_entropies(hard_sink_analyses)\n",
    "\n",
    "print(\"Extracting binary sink classifications...\")\n",
    "easy_binary_sinks = extract_binary_sink_heads(easy_sink_analyses)\n",
    "hard_binary_sinks = extract_binary_sink_heads(hard_sink_analyses)\n",
    "\n",
    "# Perform sink behavior analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WEIGHTED ENTROPY BEHAVIOR ANALYSIS - MD DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze each condition\n",
    "easy_summary = summarize_weighted_sink_behavior(easy_sink_analyses, \"EASY PROBLEMS\")\n",
    "hard_summary = summarize_weighted_sink_behavior(hard_sink_analyses, \"HARD PROBLEMS\")\n",
    "\n",
    "# Compare between conditions\n",
    "compare_weighted_sink_behavior(easy_sink_analyses, hard_sink_analyses, \"Easy\", \"Hard\")\n",
    "\n",
    "# --- Save All MD Data Arrays ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING ALL MD DATA ARRAYS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save all arrays for MD dataset\n",
    "save_weighted_entropy_arrays(\n",
    "    model_tag=MODEL_TAG,\n",
    "    dataset_tag=DATASET_TAG,\n",
    "    \n",
    "    # Weighted entropy data (supervisor's method)\n",
    "    easy_weighted_entropy=easy_weighted_extracted,\n",
    "    hard_weighted_entropy=hard_weighted_extracted,\n",
    "    \n",
    "    # Standard entropy for comparison\n",
    "    easy_standard_entropy=easy_standard_extracted,\n",
    "    hard_standard_entropy=hard_standard_extracted,\n",
    "    \n",
    "    # Sink strength values (P1)\n",
    "    easy_sink_strength=easy_sink_strengths,\n",
    "    hard_sink_strength=hard_sink_strengths,\n",
    "    \n",
    "    # Binary sink classifications\n",
    "    easy_binary_sinks=easy_binary_sinks,\n",
    "    hard_binary_sinks=hard_binary_sinks,\n",
    ")\n",
    "\n",
    "# Save detailed sink analysis results\n",
    "print(\"Saving detailed sink analysis...\")\n",
    "import pickle\n",
    "with open(f\"sink_analysis_{MODEL_TAG}_{DATASET_TAG}.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        'easy_sink_analyses': easy_sink_analyses,\n",
    "        'hard_sink_analyses': hard_sink_analyses,\n",
    "        'easy_summary': easy_summary,\n",
    "        'hard_summary': hard_summary,\n",
    "        'model_tag': MODEL_TAG,\n",
    "        'dataset_tag': DATASET_TAG,\n",
    "        'pair_ids': pair_ids_md\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nMD weighted entropy calculation and sink analysis complete for {MODEL_TAG}!\")\n",
    "\n",
    "# --- Quick Verification ---\n",
    "print(f\"\\n--- DATA SHAPE VERIFICATION ---\n",
    "print(f\"easy_weighted_entropy: {easy_weighted_extracted.shape}\")\n",
    "print(f\"hard_weighted_entropy: {hard_weighted_extracted.shape}\")\n",
    "print(f\"easy_standard_entropy: {easy_standard_extracted.shape}\")\n",
    "print(f\"hard_standard_entropy: {hard_standard_extracted.shape}\")\n",
    "print(f\"easy_sink_strength: {easy_sink_strengths.shape}\")\n",
    "print(f\"hard_sink_strength: {hard_sink_strengths.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b03cd-4e6a-4800-8adc-f31c0f3cff77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
